{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc739d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic AI News Editor: Comprehensive EDA Pipeline\n",
    "# -----------------------------------------------\n",
    "# This pipeline combines analysis of the Microsoft MIND dataset to prepare data\n",
    "# for an agentic AI system that selects, ranks, and rewrites news headlines\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "from textstat import flesch_reading_ease\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "output_dir = 'preprocessed_data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    os.makedirs(f'{output_dir}/plots')\n",
    "    os.makedirs(f'{output_dir}/processed_data')\n",
    "\n",
    "print(\"# Agentic AI News Editor - EDA Pipeline\")\n",
    "print(\"Starting EDA pipeline... Time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1689ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# PART 1: DATA LOADING AND INITIAL EXPLORATION\n",
    "# ===============================================================\n",
    "print(\"\\n## PART 1: Data Loading and Initial Exploration\")\n",
    "\n",
    "# Load news data\n",
    "news_cols = [\"newsID\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "print(\"Loading news data...\")\n",
    "news_df = pd.read_csv(\"source_data/train_data/news.tsv\", sep=\"\\t\", header=None, names=news_cols)\n",
    "print(f\"News data loaded: {news_df.shape[0]} rows, {news_df.shape[1]} columns\")\n",
    "\n",
    "# Loading behaviors data\n",
    "print(\"Loading behaviors data...\")\n",
    "behaviors_cols = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "behaviors_df = pd.read_csv(\"train_data/behaviors.tsv\", sep=\"\\t\", header=None, names=behaviors_cols)\n",
    "print(f\"Behaviors data loaded: {behaviors_df.shape[0]} rows, {behaviors_df.shape[1]} columns\")\n",
    "\n",
    "# Store news_ids for reference\n",
    "news_ids = set(news_df[\"newsID\"])\n",
    "\n",
    "# Sample display of news data\n",
    "print(\"\\nSample news data:\")\n",
    "display(news_df.head(3))\n",
    "\n",
    "# Sample display of behaviors data\n",
    "print(\"\\nSample behaviors data:\")\n",
    "display(behaviors_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# PART 2: NEWS DATA ANALYSIS\n",
    "# ===============================================================\n",
    "print(\"\\n## PART 2: News Content Analysis\")\n",
    "\n",
    "# Check for duplicates in news data\n",
    "duplicate_news = news_df.duplicated(subset=[\"newsID\"]).sum()\n",
    "print(f\"Duplicate news IDs: {duplicate_news}\")\n",
    "\n",
    "# Check for duplicate titles\n",
    "duplicate_titles = news_df.duplicated(subset=[\"title\"], keep=False).sum()\n",
    "print(f\"Duplicate news titles: {duplicate_titles}\")\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values in news data:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"Percentage of abstracts missing: {news_df['abstract'].isnull().mean()*100:.2f}%\")\n",
    "\n",
    "# Replace missing abstracts with empty string\n",
    "news_df['abstract'] = news_df['abstract'].fillna(\"\")\n",
    "news_df['title_entities'] = news_df['title_entities'].fillna(\"[]\")\n",
    "news_df['abstract_entities'] = news_df['abstract_entities'].fillna(\"[]\")\n",
    "\n",
    "# Calculate text lengths for analysis\n",
    "news_df[\"title_length\"] = news_df[\"title\"].str.len()\n",
    "news_df[\"abstract_length\"] = news_df[\"abstract\"].str.len()\n",
    "\n",
    "# Check title and abstract lengths\n",
    "print(f\"\\nVery short titles (<10 chars): {(news_df['title_length'] < 10).sum()}\")\n",
    "print(f\"Very short abstracts (<20 chars): {(news_df['abstract_length'] < 20).sum()}\")\n",
    "\n",
    "# Calculate reading ease scores for titles - key for headline rewriting task\n",
    "def calculate_reading_scores(df):\n",
    "    \"\"\"Calculate reading ease scores for titles\"\"\"\n",
    "    df['title_reading_ease'] = df['title'].apply(lambda x: flesch_reading_ease(x) if isinstance(x, str) and len(x) > 0 else np.nan)\n",
    "    return df\n",
    "\n",
    "news_df = calculate_reading_scores(news_df)\n",
    "\n",
    "print(\"\\nTitle reading ease score statistics:\")\n",
    "print(news_df['title_reading_ease'].describe())\n",
    "\n",
    "# Analyze category and subcategory distributions\n",
    "print(\"\\nCategory distribution:\")\n",
    "category_counts = news_df[\"category\"].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Create visualization of category distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = category_counts.plot(kind='bar')\n",
    "plt.title('News Articles by Category', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plots/category_distribution.png')\n",
    "\n",
    "# Analyze subcategory distributions for top categories\n",
    "top_categories = category_counts.index[:5].tolist()\n",
    "for category in top_categories:\n",
    "    subcat_counts = news_df[news_df['category'] == category]['subcategory'].value_counts().head(10)\n",
    "    print(f\"\\nTop 10 subcategories for {category}:\")\n",
    "    print(subcat_counts)\n",
    "    \n",
    "    # Visualize subcategory distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    subcat_counts.plot(kind='bar')\n",
    "    plt.title(f'Top 10 Subcategories for {category}', fontsize=16)\n",
    "    plt.xlabel('Subcategory', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/plots/subcategory_{category}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# PART 3: USER BEHAVIOR ANALYSIS\n",
    "# ===============================================================\n",
    "print(\"\\n## PART 3: User Behavior Analysis\")\n",
    "\n",
    "# Check for duplicates and missing values in behaviors data\n",
    "duplicate_impressions = behaviors_df.duplicated(subset=[\"impression_id\"]).sum()\n",
    "print(f\"Duplicate impression IDs: {duplicate_impressions}\")\n",
    "print(\"\\nMissing values in behaviors data:\")\n",
    "print(behaviors_df.isnull().sum())\n",
    "\n",
    "# Calculate percentage of missing history data\n",
    "missing_history_count = behaviors_df['history'].isna().sum()\n",
    "missing_history_pct = (missing_history_count / len(behaviors_df)) * 100\n",
    "print(f\"Missing history: {missing_history_count} rows ({missing_history_pct:.2f}%)\")\n",
    "\n",
    "# Analyze users with missing history\n",
    "missing_history_df = behaviors_df[behaviors_df['history'].isna()]\n",
    "unique_users_missing = missing_history_df['user_id'].nunique()\n",
    "print(f\"Unique users with missing history: {unique_users_missing}\")\n",
    "\n",
    "# Process impressions data to analyze clicks and user engagement\n",
    "print(\"\\nProcessing impressions data...\")\n",
    "\n",
    "def process_impressions(df, sample_size=None):\n",
    "    \"\"\"Convert the impressions data into a flattened format for analysis\"\"\"\n",
    "    if sample_size:\n",
    "        df = df.head(sample_size)\n",
    "    \n",
    "    impressions_expanded = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            impressions = row['impressions'].split()\n",
    "            for item in impressions:\n",
    "                if '-' in item:\n",
    "                    news_id, clicked = item.split('-')\n",
    "                    impressions_expanded.append({\n",
    "                        'impression_id': row['impression_id'],\n",
    "                        'user_id': row['user_id'],\n",
    "                        'news_id': news_id,\n",
    "                        'clicked': int(clicked)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping row due to error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(impressions_expanded)\n",
    "\n",
    "impressions_df = process_impressions(behaviors_df)\n",
    "print(f\"Expanded to {len(impressions_df)} impression records\")\n",
    "\n",
    "# Check for invalid news IDs (not in news_df)\n",
    "invalid_news_ids = impressions_df[~impressions_df[\"news_id\"].isin(news_ids)]\n",
    "print(f\"Impression records with invalid news IDs: {len(invalid_news_ids)}\")\n",
    "\n",
    "# Analyze overall click-through rate\n",
    "clicks = impressions_df[\"clicked\"].sum()\n",
    "total = len(impressions_df)\n",
    "print(f\"\\nOverall CTR: {clicks/total:.4f} ({clicks} clicks out of {total} impressions)\")\n",
    "\n",
    "# Analyze articles with very few impressions (potentially unreliable CTR)\n",
    "article_impressions = impressions_df.groupby(\"news_id\").size()\n",
    "low_impression_articles = (article_impressions < 5).sum()\n",
    "print(f\"Articles with fewer than 5 impressions: {low_impression_articles}\")\n",
    "\n",
    "# Check for extreme CTRs (potential data issues)\n",
    "article_ctrs = impressions_df.groupby(\"news_id\")[\"clicked\"].mean()\n",
    "suspicious_ctrs = ((article_ctrs == 0) | (article_ctrs > 0.7)).sum()\n",
    "print(f\"Articles with suspicious CTRs (0 or >70%): {suspicious_ctrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# PART 4: ANALYSIS RELEVANT TO EDITORIAL TASKS\n",
    "# ===============================================================\n",
    "print(\"\\n## PART 4: Analysis for Editorial Tasks\")\n",
    "\n",
    "# Merge impressions with news data to analyze CTR by category\n",
    "print(\"\\nAnalyzing click-through rates by news category...\")\n",
    "merged_df = impressions_df.merge(\n",
    "    news_df[['newsID', 'category', 'subcategory', 'title', 'title_reading_ease', 'title_length']], \n",
    "    left_on='news_id', \n",
    "    right_on='newsID'\n",
    ")\n",
    "\n",
    "# Calculate CTR by category\n",
    "category_ctr = merged_df.groupby('category').agg({\n",
    "    'clicked': ['mean', 'std', 'count']\n",
    "})\n",
    "category_ctr.columns = ['ctr_mean', 'ctr_std', 'impression_count']\n",
    "category_ctr = category_ctr.sort_values('ctr_mean', ascending=False)\n",
    "\n",
    "print(\"\\nCTR by category:\")\n",
    "print(category_ctr)\n",
    "\n",
    "# Visualize CTR by category\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.barplot(x=category_ctr.index, y=category_ctr['ctr_mean'])\n",
    "plt.title('Click-Through Rate by News Category', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=14)\n",
    "plt.ylabel('CTR (Average)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(category_ctr['impression_count']):\n",
    "    ax.text(i, 0.005, f'n={count}', ha='center', va='bottom', color='black', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plots/category_ctr.png')\n",
    "\n",
    "# Statistical test for category CTR differences\n",
    "categories = []\n",
    "ctr_values = []\n",
    "for category, group in merged_df.groupby('category'):\n",
    "    if len(group) >= 30:  # Only include categories with sufficient data\n",
    "        categories.append(category)\n",
    "        ctr_values.append(group['clicked'].values)\n",
    "\n",
    "if len(categories) >= 2:\n",
    "    f_stat, p_value = stats.f_oneway(*ctr_values)\n",
    "    print(f\"\\nANOVA test for CTR differences between categories:\")\n",
    "    print(f\"F={f_stat:.4f}, p={p_value:.6f}\")\n",
    "    significance = \"Significant\" if p_value < 0.05 else \"Not significant\"\n",
    "    print(f\"Result: {significance} differences in CTR across categories\")\n",
    "\n",
    "# Analyze correlation between title properties and CTR\n",
    "print(\"\\nAnalyzing relationship between title properties and engagement...\")\n",
    "\n",
    "# Group by news_id to get average CTR per article\n",
    "article_stats = merged_df.groupby('newsID').agg({\n",
    "    'clicked': 'mean',\n",
    "    'title_length': 'first',\n",
    "    'title_reading_ease': 'first'\n",
    "}).rename(columns={'clicked': 'ctr'})\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = article_stats[['ctr', 'title_length', 'title_reading_ease']].corr()\n",
    "print(\"\\nCorrelations between article properties and CTR:\")\n",
    "print(correlations['ctr'].sort_values(ascending=False))\n",
    "\n",
    "# Visualize relationship between reading ease and CTR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(article_stats['title_reading_ease'], article_stats['ctr'], alpha=0.3)\n",
    "plt.title('Relationship Between Title Reading Ease and CTR', fontsize=16)\n",
    "plt.xlabel('Flesch Reading Ease Score', fontsize=14)\n",
    "plt.ylabel('Click-Through Rate', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/plots/reading_ease_vs_ctr.png')\n",
    "\n",
    "# Analyze headline patterns - can help with rewriting task\n",
    "print(\"\\nAnalyzing headline patterns...\")\n",
    "\n",
    "# Function to detect headline patterns\n",
    "def analyze_headline_patterns(titles):\n",
    "    patterns = {\n",
    "        'questions': 0,\n",
    "        'numbers': 0,\n",
    "        'quotes': 0,\n",
    "        'colons': 0,\n",
    "        'ellipsis': 0\n",
    "    }\n",
    "    \n",
    "    for title in titles:\n",
    "        if '?' in title:\n",
    "            patterns['questions'] += 1\n",
    "        if any(char.isdigit() for char in title):\n",
    "            patterns['numbers'] += 1\n",
    "        if '\"' in title or \"'\" in title:\n",
    "            patterns['quotes'] += 1\n",
    "        if ':' in title:\n",
    "            patterns['colons'] += 1\n",
    "        if '...' in title:\n",
    "            patterns['ellipsis'] += 1\n",
    "    \n",
    "    total = len(titles)\n",
    "    return {k: (v, v/total*100) for k, v in patterns.items()}\n",
    "\n",
    "headline_patterns = analyze_headline_patterns(news_df['title'])\n",
    "print(\"\\nHeadline patterns:\")\n",
    "for pattern, (count, percentage) in headline_patterns.items():\n",
    "    print(f\"{pattern}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Analyze headline patterns by engagement\n",
    "high_ctr_articles = article_stats[article_stats['ctr'] > article_stats['ctr'].median()]\n",
    "low_ctr_articles = article_stats[article_stats['ctr'] <= article_stats['ctr'].median()]\n",
    "\n",
    "high_ctr_news_ids = high_ctr_articles.index.tolist()\n",
    "low_ctr_news_ids = low_ctr_articles.index.tolist()\n",
    "\n",
    "high_ctr_titles = news_df[news_df['newsID'].isin(high_ctr_news_ids)]['title']\n",
    "low_ctr_titles = news_df[news_df['newsID'].isin(low_ctr_news_ids)]['title']\n",
    "\n",
    "high_ctr_patterns = analyze_headline_patterns(high_ctr_titles)\n",
    "low_ctr_patterns = analyze_headline_patterns(low_ctr_titles)\n",
    "\n",
    "print(\"\\nHeadline patterns in high-engagement articles:\")\n",
    "for pattern, (count, percentage) in high_ctr_patterns.items():\n",
    "    print(f\"{pattern}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nHeadline patterns in low-engagement articles:\")\n",
    "for pattern, (count, percentage) in low_ctr_patterns.items():\n",
    "    print(f\"{pattern}: {count} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# PART 5: PREPARE DATA FOR THE AGENTIC NEWS EDITOR\n",
    "# ===============================================================\n",
    "print(\"\\n## PART 5: Data Preparation for Agentic News Editor\")\n",
    "\n",
    "# 1. Clean and process news data\n",
    "print(\"Preparing news data...\")\n",
    "news_df_cleaned = news_df.copy()\n",
    "\n",
    "# Filter out articles with very short titles or abstracts\n",
    "news_df_cleaned = news_df_cleaned[news_df_cleaned['title_length'] >= 10]\n",
    "news_df_cleaned = news_df_cleaned[news_df_cleaned['abstract_length'] >= 20]\n",
    "\n",
    "# Add reading ease score if not already added\n",
    "if 'title_reading_ease' not in news_df_cleaned.columns:\n",
    "    news_df_cleaned = calculate_reading_scores(news_df_cleaned)\n",
    "\n",
    "# 2. Prepare behavior data\n",
    "print(\"Preparing behavior data...\")\n",
    "\n",
    "# Add CTR data to news articles\n",
    "article_ctr_data = impressions_df.groupby('news_id').agg({\n",
    "    'clicked': ['sum', 'count']\n",
    "})\n",
    "article_ctr_data.columns = ['total_clicks', 'total_impressions']\n",
    "article_ctr_data['ctr'] = article_ctr_data['total_clicks'] / article_ctr_data['total_impressions']\n",
    "\n",
    "# Merge CTR data with news data\n",
    "news_with_engagement = news_df_cleaned.merge(\n",
    "    article_ctr_data.reset_index(),\n",
    "    left_on='newsID',\n",
    "    right_on='news_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing engagement data with zeros\n",
    "engagement_columns = ['total_clicks', 'total_impressions', 'ctr']\n",
    "news_with_engagement[engagement_columns] = news_with_engagement[engagement_columns].fillna(0)\n",
    "\n",
    "# 3. Create dataset for title rewriting task\n",
    "print(\"Creating dataset for headline rewriting task...\")\n",
    "\n",
    "# Filter to articles with sufficient impressions for reliable CTR data\n",
    "rewriting_candidates = news_with_engagement[news_with_engagement['total_impressions'] >= 5].copy()\n",
    "\n",
    "# Group titles by reading ease score to identify good and bad examples\n",
    "rewriting_candidates['reading_ease_bin'] = pd.qcut(\n",
    "    rewriting_candidates['title_reading_ease'], \n",
    "    q=5, \n",
    "    labels=['Very Hard', 'Hard', 'Medium', 'Easy', 'Very Easy']\n",
    ")\n",
    "\n",
    "# Create a dataset to show examples from different readability levels\n",
    "rewriting_examples = {}\n",
    "for readability_level in rewriting_candidates['reading_ease_bin'].unique():\n",
    "    group_df = rewriting_candidates[rewriting_candidates['reading_ease_bin'] == readability_level]\n",
    "    \n",
    "    # Get high and low CTR examples from this readability group\n",
    "    high_ctr = group_df.nlargest(5, 'ctr')\n",
    "    low_ctr = group_df.nsmallest(5, 'ctr')\n",
    "    \n",
    "    rewriting_examples[readability_level] = {\n",
    "        'high_ctr': high_ctr[['newsID', 'title', 'category', 'ctr', 'title_reading_ease']].to_dict('records'),\n",
    "        'low_ctr': low_ctr[['newsID', 'title', 'category', 'ctr', 'title_reading_ease']].to_dict('records')\n",
    "    }\n",
    "\n",
    "# 4. Save processed data for the news editor system\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save the cleaned news data with engagement metrics\n",
    "news_with_engagement.to_csv(f'{output_dir}/processed_data/news_with_engagement.csv', index=False)\n",
    "\n",
    "# Save headline rewriting examples dataset\n",
    "with open(f'{output_dir}/processed_data/headline_rewriting_examples.json', 'w') as f:\n",
    "    json.dump(rewriting_examples, f, indent=2)\n",
    "\n",
    "# Save category and subcategory distributions for editorial diversity goals\n",
    "category_distribution = news_df['category'].value_counts().to_dict()\n",
    "with open(f'{output_dir}/processed_data/category_distribution.json', 'w') as f:\n",
    "    json.dump(category_distribution, f, indent=2)\n",
    "\n",
    "# Create an editorial guideline summary based on the analysis\n",
    "category_ctr_summary = category_ctr.reset_index().to_dict('records')\n",
    "headline_insights = {\n",
    "    'reading_ease_correlation': correlations['ctr']['title_reading_ease'],\n",
    "    'headline_patterns': {\n",
    "        'high_engagement': high_ctr_patterns,\n",
    "        'low_engagement': low_ctr_patterns\n",
    "    }\n",
    "}\n",
    "\n",
    "editorial_guidelines = {\n",
    "    'category_performance': category_ctr_summary,\n",
    "    'headline_insights': headline_insights,\n",
    "    'overall_ctr_benchmark': clicks/total\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/processed_data/editorial_guidelines.json', 'w') as f:\n",
    "    json.dump(editorial_guidelines, f, indent=2)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"EDA Pipeline completed! Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Processed data saved to '{output_dir}/processed_data/'\")\n",
    "print(f\"Visualizations saved to '{output_dir}/plots/'\")\n",
    "print(f\"Ready for Agentic AI News Editor development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975dcae",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f78acb",
   "metadata": {},
   "source": [
    "Questions in headlines significantly reduce CTR (↓27.9%, p < 0.001) — best to avoid.\n",
    "\n",
    "Numbers in headlines slightly reduce CTR (↓9.8%, p < 0.05) — use sparingly and only when truly meaningful.\n",
    "\n",
    "Flesch Reading Ease Score shows no meaningful correlation with CTR — title readability (as measured here) doesn’t drive clicks.\n",
    "\n",
    "News categories have a strong effect on CTR:\n",
    "\n",
    "High CTR: kids, music, tv\n",
    "\n",
    "Low CTR: autos, travel, northamerica\n",
    "\n",
    "Headline patterns:\n",
    "\n",
    "High CTR headlines slightly more often contain quotes or numbers.\n",
    "\n",
    "Low CTR headlines slightly more often contain questions and colons — again reinforcing that questions may underperform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ee4f6",
   "metadata": {},
   "source": [
    "Flesch Reading Ease Score\n",
    "A score from ~0 to 100+ that measures how easy a text is to read.\n",
    "Higher score → easier to read.\n",
    "Score ~60–70 = 8th–9th grade level (standard online content).\n",
    "Negative scores → likely errors or very dense content.\n",
    "Formula considers average sentence length and average syllables per word.\n",
    "In our data: Not predictive of CTR → readers may be more influenced by content topic and style than raw readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e7b3c",
   "metadata": {},
   "source": [
    "Prompt Guidelines for LLM-Generated Headlines to increase CTR\n",
    "\n",
    "Use informative, clear titles without unnecessary complexity.\n",
    "Favor straightforward declarative formats (not questions).\n",
    "Match tone and content to high-performing categories (e.g., music, TV).\n",
    "Consider including emotion, surprise, or curiosity (instead of lists or questions).\n",
    "\n",
    "Avoid:\n",
    "Starting with \"Is\", \"What\", \"How\" — question formats underperform.\n",
    "Relying on \"Top 5\", \"10 Ways\", etc. unless it directly adds value.\n",
    "Overcomplicated or overly “clever” phrasing (no CTR benefit).\n",
    "\n",
    "Baseline Template:\n",
    "Generate a short, compelling headline for a [news category] article. Avoid using questions or numbers. Aim for a tone that is clear, direct, and engaging.\n",
    "Write a headline for an article in the [sports/music/TV] category. Keep it under 12 words, use plain language, and avoid questions. Use quotes or strong verbs to capture attention.\n",
    "Sanitizing Existing Headlines:\n",
    "Rewrite this headline to increase click-through rate:\n",
    "\"Is Climate Change Getting Worse in 2024?\"\n",
    "Avoid questions\n",
    "Avoid numbers\n",
    "Match tone to a general audience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd6def",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
