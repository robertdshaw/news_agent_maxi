import pandas as pd
import pickle
from pathlib import Path

print("Fixing missing embeddings by merging cached embeddings into parquet files...")

# Paths
PREP_DIR = Path("data/preprocessed")
cache_dir = PREP_DIR / "cache"

# Load current parquet files
print("Loading current parquet files...")
X_train = pd.read_parquet(PREP_DIR / "processed_data" / "X_train.parquet")
X_val = pd.read_parquet(PREP_DIR / "processed_data" / "X_val.parquet")
X_test = pd.read_parquet(PREP_DIR / "processed_data" / "X_test.parquet")

print(f"Current feature counts:")
print(f"- Train: {len(X_train.columns)} features")
print(f"- Val: {len(X_val.columns)} features")
print(f"- Test: {len(X_test.columns)} features")

# Check if we have a newsID column to merge on
print(f"\nChecking merge keys...")
train_merge_key = None
val_merge_key = None
test_merge_key = None

# Look for possible merge keys
possible_keys = ["newsID", "news_id", "index"]
for key in possible_keys:
    if key in X_train.columns:
        train_merge_key = key
        break

for key in possible_keys:
    if key in X_val.columns:
        val_merge_key = key
        break

for key in possible_keys:
    if key in X_test.columns:
        test_merge_key = key
        break

print(
    f"Merge keys found: Train={train_merge_key}, Val={val_merge_key}, Test={test_merge_key}"
)

# Load cached embeddings
print("\nLoading cached embeddings...")

# Train embeddings
train_emb_path = cache_dir / "train_embeddings.pkl"
if train_emb_path.exists():
    with open(train_emb_path, "rb") as f:
        train_emb_map = pickle.load(f)
    print(
        f"- Train embeddings: {train_emb_map.shape} (index type: {type(train_emb_map.index)})"
    )
else:
    print("- Train embeddings: NOT FOUND")
    train_emb_map = None

# Val embeddings
val_emb_path = cache_dir / "val_embeddings.pkl"
if val_emb_path.exists():
    with open(val_emb_path, "rb") as f:
        val_emb_map = pickle.load(f)
    print(
        f"- Val embeddings: {val_emb_map.shape} (index type: {type(val_emb_map.index)})"
    )
else:
    print("- Val embeddings: NOT FOUND")
    val_emb_map = None

# Test embeddings
test_emb_path = cache_dir / "test_embeddings.pkl"
if test_emb_path.exists():
    with open(test_emb_path, "rb") as f:
        test_emb_map = pickle.load(f)
    print(
        f"- Test embeddings: {test_emb_map.shape} (index type: {type(test_emb_map.index)})"
    )
else:
    print("- Test embeddings: NOT FOUND")
    test_emb_map = None

# Merge embeddings into parquet files
print(f"\nMerging embeddings...")

# Method 1: If we have merge keys, use them
if train_merge_key and train_emb_map is not None:
    print(f"Merging train embeddings using {train_merge_key}...")
    X_train_with_emb = X_train.merge(
        train_emb_map, left_on=train_merge_key, right_index=True, how="left"
    )
    print(
        f"Train merge result: {len(X_train_with_emb.columns)} features ({len(X_train_with_emb.columns) - len(X_train.columns)} embeddings added)"
    )
    X_train = X_train_with_emb

if val_merge_key and val_emb_map is not None:
    print(f"Merging val embeddings using {val_merge_key}...")
    X_val_with_emb = X_val.merge(
        val_emb_map, left_on=val_merge_key, right_index=True, how="left"
    )
    print(
        f"Val merge result: {len(X_val_with_emb.columns)} features ({len(X_val_with_emb.columns) - len(X_val.columns)} embeddings added)"
    )
    X_val = X_val_with_emb

if test_merge_key and test_emb_map is not None:
    print(f"Merging test embeddings using {test_merge_key}...")
    X_test_with_emb = X_test.merge(
        test_emb_map, left_on=test_merge_key, right_index=True, how="left"
    )
    print(
        f"Test merge result: {len(X_test_with_emb.columns)} features ({len(X_test_with_emb.columns) - len(X_test.columns)} embeddings added)"
    )
    X_test = X_test_with_emb

# Method 2: If no merge keys, try index-based merge (risky but might work)
if train_merge_key is None and train_emb_map is not None:
    print("WARNING: No merge key found for train, attempting index-based merge...")
    print(
        f"Train parquet index: {len(X_train)}, Embeddings index: {len(train_emb_map)}"
    )
    if len(X_train) <= len(train_emb_map):
        # Take first N embeddings
        emb_subset = train_emb_map.iloc[: len(X_train)].reset_index(drop=True)
        X_train = pd.concat([X_train.reset_index(drop=True), emb_subset], axis=1)
        print(f"Index-based merge completed: {len(X_train.columns)} features")

# Add interaction terms (since they were missing)
print(f"\nAdding interaction terms...")
interaction_features = []

for df_name, df in [("train", X_train), ("val", X_val), ("test", X_test)]:
    if "title_length" in df.columns and "category_enc" in df.columns:
        df["title_length_x_category"] = df["title_length"] * df["category_enc"]
        df["has_colon_x_category"] = df["has_colon"] * df["category_enc"]
        df["word_count_x_category"] = df["title_word_count"] * df["category_enc"]
        df["has_quotes_x_category"] = df["has_quotes"] * df["category_enc"]
        df["title_features_combined"] = (
            df["has_colon"] + df["has_quotes"] + df["has_number"]
        ) * df["title_length"]

        if df_name == "train":
            interaction_features = [
                "title_length_x_category",
                "has_colon_x_category",
                "word_count_x_category",
                "has_quotes_x_category",
                "title_features_combined",
            ]

print(f"Added {len(interaction_features)} interaction features")

# Save updated parquet files
print(f"\nSaving updated parquet files...")
X_train.to_parquet(PREP_DIR / "processed_data" / "X_train.parquet")
X_val.to_parquet(PREP_DIR / "processed_data" / "X_val.parquet")
X_test.to_parquet(PREP_DIR / "processed_data" / "X_test.parquet")

print(f"\nFinal feature counts:")
print(f"- Train: {len(X_train.columns)} features")
print(f"- Val: {len(X_val.columns)} features")
print(f"- Test: {len(X_test.columns)} features")

# Count embedding features
emb_features = [col for col in X_train.columns if col.startswith("emb_")]
print(f"- Embedding features: {len(emb_features)}")

# Show sample of feature types
print(f"\nFeature breakdown:")
feature_types = {
    "text": [
        col
        for col in X_train.columns
        if any(x in col for x in ["title_", "abstract_", "has_"])
    ],
    "temporal": [
        col
        for col in X_train.columns
        if any(x in col for x in ["hour", "day_", "weekend", "time_"])
    ],
    "interaction": [col for col in X_train.columns if "_x_" in col],
    "embedding": [col for col in X_train.columns if col.startswith("emb_")],
    "other": [
        col
        for col in X_train.columns
        if not any(
            x in col
            for x in [
                "title_",
                "abstract_",
                "has_",
                "hour",
                "day_",
                "weekend",
                "time_",
                "_x_",
                "emb_",
            ]
        )
    ],
}

for ftype, features in feature_types.items():
    print(f"  {ftype}: {len(features)} features")

# Test correlations with embeddings
if emb_features:
    print(f"\nTesting embedding correlations...")
    y_train = pd.read_parquet(PREP_DIR / "processed_data" / "y_train.parquet")["ctr"]
    emb_corrs = (
        X_train[emb_features].corrwith(y_train).abs().sort_values(ascending=False)
    )
    print(f"Top 5 embedding correlations:")
    print(emb_corrs.head())

print(f"\n✅ Fixed! Your data now has embeddings and interaction features.")
print(f"✅ Ready for model training with {len(X_train.columns)} total features.")
