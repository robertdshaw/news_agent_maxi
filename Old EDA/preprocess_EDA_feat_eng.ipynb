{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0177b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshaw\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports & Paths\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textstat import flesch_reading_ease\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Raw-data directories\n",
    "TRAIN_DIR = Path(\"source_data/train_data\")\n",
    "VAL_DIR   = Path(\"source_data/val_data\")\n",
    "TEST_DIR  = Path(\"source_data/test_data\")\n",
    "\n",
    "# Output directories\n",
    "PREP_DIR = Path(\"data/preprocessed\")\n",
    "PREP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(PREP_DIR / \"plots\").mkdir(exist_ok=True)\n",
    "(PREP_DIR / \"processed_data\").mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df823e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def calc_reading(text):\n",
    "    return flesch_reading_ease(text) if isinstance(text, str) and text.strip() else 0\n",
    "\n",
    "def make_basic_text_feats(df):\n",
    "    df = df.copy()\n",
    "    df[\"title_length\"]    = df[\"title\"].str.len()\n",
    "    df[\"abstract_length\"] = df[\"abstract\"].fillna(\"\").str.len()\n",
    "    df[\"title_reading_ease\"] = df[\"title\"].apply(calc_reading)\n",
    "    df[\"has_question\"]    = df[\"title\"].str.contains(r\"\\?\").astype(int)\n",
    "    df[\"has_exclamation\"] = df[\"title\"].str.contains(r\"!\").astype(int)\n",
    "    df[\"has_number\"]      = df[\"title\"].str.contains(r\"\\d\").astype(int)\n",
    "    df[\"has_colon\"]       = df[\"title\"].str.contains(r\":\").astype(int)\n",
    "    df[\"has_quotes\"]      = df[\"title\"].str.contains(r'[\"\\']').astype(int)\n",
    "    return df\n",
    "\n",
    "def make_time_feats(df):\n",
    "    df = df.copy()\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df[\"hour\"]        = df[\"time\"].dt.hour.fillna(0).astype(int)\n",
    "    df[\"day_of_week\"] = df[\"time\"].dt.dayofweek.fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "def make_embeddings(df, embedder, dim=50):\n",
    "    embs = embedder.encode(df[\"title\"].tolist(), show_progress_bar=True)\n",
    "    return pd.DataFrame(embs[:, :dim],\n",
    "                        columns=[f\"emb_{i}\" for i in range(dim)],\n",
    "                        index=df.index)\n",
    "\n",
    "def process_impressions(df, sample_size=None):\n",
    "    # 1) Sample safely\n",
    "    df2 = df.sample(n=min(sample_size or len(df), len(df)), random_state=42).copy()\n",
    "    # 2) Explode the space-separated impressions\n",
    "    df2[\"impressions\"] = df2[\"impressions\"].str.split()\n",
    "    df2 = df2.explode(\"impressions\").reset_index(drop=True)\n",
    "    # 3) Keep only well-formed entries\n",
    "    mask = df2[\"impressions\"].str.contains(r'^[^-]+-[01]$')\n",
    "    df2 = df2.loc[mask]\n",
    "    # 4) If nothing left, return empty with the right cols\n",
    "    if df2.empty:\n",
    "        return pd.DataFrame(columns=[\"news_id\", \"clicked\"])\n",
    "    # 5) Split into news_id & clicked\n",
    "    split_df = df2[\"impressions\"].str.split(\"-\", n=1, expand=True)\n",
    "    split_df.columns = [\"news_id\", \"clicked_str\"]\n",
    "    df2 = df2.loc[split_df.index]\n",
    "    df2[\"news_id\"] = split_df[\"news_id\"]\n",
    "    df2[\"clicked\"] = split_df[\"clicked_str\"].astype(int)\n",
    "    # 6) Drop helper cols, ignoring if they don’t exist\n",
    "    return df2.drop(columns=[\"impressions\", \"clicked_str\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8fbb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshaw\\AppData\\Local\\Temp\\ipykernel_16408\\1677372064.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[[\"total_clicks\",\"total_impressions\",\"ctr\"]].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 101527/72023/120959\n"
     ]
    }
   ],
   "source": [
    "# Load Raw Splits & Compute CTR\n",
    "\n",
    "def load_and_label(split_dir):\n",
    "    news = pd.read_csv(split_dir / \"news.tsv\", sep=\"\\t\", header=None,\n",
    "                       names=[\"newsID\",\"category\",\"subcategory\",\"title\",\n",
    "                              \"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "    news[\"abstract\"] = news[\"abstract\"].fillna(\"\")\n",
    "    beh = pd.read_csv(split_dir / \"behaviors.tsv\", sep=\"\\t\", header=None,\n",
    "                      names=[\"impression_id\",\"user_id\",\"time\",\"history\",\"impressions\"])\n",
    "    imps = process_impressions(beh, sample_size=800_000)\n",
    "    agg = (imps.groupby(\"news_id\")\n",
    "               .agg(total_clicks    = (\"clicked\",\"sum\"),\n",
    "                    total_impressions=(\"clicked\",\"count\"))\n",
    "               .assign(ctr=lambda d: d.total_clicks / d.total_impressions)\n",
    "               .reset_index())\n",
    "    merged = news.merge(agg, left_on=\"newsID\", right_on=\"news_id\", how=\"left\")\n",
    "    merged[[\"total_clicks\",\"total_impressions\",\"ctr\"]] = \\\n",
    "      merged[[\"total_clicks\",\"total_impressions\",\"ctr\"]].fillna(0)\n",
    "    return merged\n",
    "\n",
    "df_train = load_and_label(TRAIN_DIR)\n",
    "df_val   = load_and_label(VAL_DIR)\n",
    "df_test  = load_and_label(TEST_DIR)\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(df_train)}/{len(df_val)}/{len(df_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c69ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set:\n",
      "Missing values per column:\n",
      "newsID                   0\n",
      "category                 0\n",
      "subcategory              0\n",
      "title                    0\n",
      "abstract                 0\n",
      "url                      0\n",
      "title_entities           3\n",
      "abstract_entities        6\n",
      "news_id              76393\n",
      "total_clicks             0\n",
      "total_impressions        0\n",
      "ctr                      0\n",
      "dtype: int64\n",
      "Rows with ≥1 missing value: 76395\n",
      "\n",
      "val set:\n",
      "Missing values per column:\n",
      "newsID                   0\n",
      "category                 0\n",
      "subcategory              0\n",
      "title                    0\n",
      "abstract                 0\n",
      "url                      0\n",
      "title_entities           2\n",
      "abstract_entities        5\n",
      "news_id              65026\n",
      "total_clicks             0\n",
      "total_impressions        0\n",
      "ctr                      0\n",
      "dtype: int64\n",
      "Rows with ≥1 missing value: 65026\n",
      "\n",
      "test set:\n",
      "Missing values per column:\n",
      "newsID                    0\n",
      "category                  0\n",
      "subcategory               0\n",
      "title                     0\n",
      "abstract                  0\n",
      "url                       1\n",
      "title_entities            6\n",
      "abstract_entities         9\n",
      "news_id              120959\n",
      "total_clicks              0\n",
      "total_impressions         0\n",
      "ctr                       0\n",
      "dtype: int64\n",
      "Rows with ≥1 missing value: 120959\n",
      "\n",
      "train set:\n",
      "Exact duplicate rows: 0\n",
      "\n",
      "val set:\n",
      "Exact duplicate rows: 0\n",
      "\n",
      "test set:\n",
      "Exact duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Overview of nulls\n",
    "for name, df in ((\"train\", df_train), (\"val\", df_val), (\"test\", df_test)):\n",
    "    print(f\"\\n{name} set:\")\n",
    "    # Total missing per column\n",
    "    print(\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    # Optionally, total rows with any missing value\n",
    "    print(\"Rows with ≥1 missing value:\", df.isnull().any(axis=1).sum())\n",
    "\n",
    "# 2. Overview of duplicates\n",
    "for name, df in ((\"train\", df_train), (\"val\", df_val), (\"test\", df_test)):\n",
    "    print(f\"\\n{name} set:\")\n",
    "    # How many exact-duplicate rows?\n",
    "    n_dup = df.duplicated().sum()\n",
    "    print(f\"Exact duplicate rows: {n_dup}\")\n",
    "    if n_dup:\n",
    "        # Show the first few duplicate rows\n",
    "        print(df[df.duplicated()].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830785e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Plots & Analysis (Train Split)\n",
    "\n",
    "# 1) Category Distribution\n",
    "cat_counts = df_train[\"category\"].value_counts()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(cat_counts.index, cat_counts.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Articles by Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PREP_DIR/\"plots\"/\"category_dist.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Title & Abstract Length Distributions\n",
    "plt.figure()\n",
    "plt.hist(df_train[\"title_length\"], bins=50)\n",
    "plt.title(\"Title Lengths\")\n",
    "plt.xlabel(\"Chars\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_train[\"abstract_length\"], bins=50)\n",
    "plt.title(\"Abstract Lengths\")\n",
    "plt.xlabel(\"Chars\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Reading Ease vs. CTR\n",
    "plt.figure()\n",
    "plt.scatter(df_train[\"title_reading_ease\"], df_train[\"ctr\"], alpha=0.3)\n",
    "plt.xlabel(\"Reading Ease\")\n",
    "plt.ylabel(\"CTR\")\n",
    "plt.title(\"ReadEase vs CTR\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Per-Article CTR Distribution\n",
    "article_ctrs = df_train.groupby(\"newsID\")[\"ctr\"].first()\n",
    "plt.figure()\n",
    "plt.hist(article_ctrs, bins=50)\n",
    "plt.title(\"Per-Article CTR\")\n",
    "plt.xlabel(\"CTR\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Impressions per Article (log scale)\n",
    "imps_per = df_train[\"total_impressions\"]\n",
    "plt.figure()\n",
    "plt.hist(imps_per, bins=50, log=True)\n",
    "plt.title(\"Impressions per Article (log)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24355c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Basic text & time\n",
    "df_train = make_basic_text_feats(df_train)\n",
    "df_train = make_time_feats(df_train)\n",
    "df_val   = make_basic_text_feats(df_val)\n",
    "df_val   = make_time_feats(df_val)\n",
    "df_test  = make_basic_text_feats(df_test)\n",
    "df_test  = make_time_feats(df_test)\n",
    "\n",
    "# Category encoding\n",
    "le = LabelEncoder()\n",
    "df_train[\"category_enc\"] = le.fit_transform(df_train[\"category\"])\n",
    "df_val[\"category_enc\"]   = le.transform(df_val[\"category\"])\n",
    "df_test[\"category_enc\"]  = le.transform(df_test[\"category\"])\n",
    "\n",
    "# Embeddings\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "emb_train = make_embeddings(df_train, embedder)\n",
    "emb_val   = make_embeddings(df_val,   embedder)\n",
    "emb_test  = make_embeddings(df_test,  embedder)\n",
    "\n",
    "# Assemble features & targets\n",
    "feat_cols = [\n",
    "    \"title_length\",\"abstract_length\",\"title_reading_ease\",\n",
    "    \"has_question\",\"has_exclamation\",\"has_number\",\n",
    "    \"has_colon\",\"has_quotes\",\"hour\",\"day_of_week\",\"category_enc\"\n",
    "] + [f\"emb_{i}\" for i in range(emb_train.shape[1])]\n",
    "\n",
    "X_train = pd.concat([df_train[feat_cols], emb_train], axis=1)\n",
    "y_train = df_train[\"ctr\"]\n",
    "X_val   = pd.concat([df_val[feat_cols],   emb_val],   axis=1)\n",
    "y_val   = df_val[\"ctr\"]\n",
    "X_test  = pd.concat([df_test[feat_cols],  emb_test],  axis=1)\n",
    "y_test  = df_test[\"ctr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c36f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Preprocessed Feature Tables\n",
    "\n",
    "pd.concat([X_train, y_train], axis=1).to_csv(PREP_DIR/\"train_feats.csv\", index=False)\n",
    "pd.concat([X_val,   y_val],   axis=1).to_csv(PREP_DIR/\"val_feats.csv\",   index=False)\n",
    "pd.concat([X_test,  y_test],  axis=1).to_csv(PREP_DIR/\"test_feats.csv\",  index=False)\n",
    "print(\"Feature CSVs written to\", PREP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & Save Editorial Guidelines\n",
    "\n",
    "merged = df_train.merge(df_train[[\"newsID\",\"category\"]], on=\"newsID\")\n",
    "cat_stats = (\n",
    "    merged.groupby(\"category\")\n",
    "          .agg(\n",
    "              ctr_mean       = (\"ctr\",\"mean\"),\n",
    "              ctr_std        = (\"ctr\",\"std\"),\n",
    "              impressions    = (\"total_impressions\",\"sum\")\n",
    "          )\n",
    "          .reset_index()\n",
    "          .to_dict(\"records\")\n",
    ")\n",
    "\n",
    "stats_df = df_train[[\"ctr\",\"title_length\",\"title_reading_ease\"]].dropna()\n",
    "corrs = stats_df.corr().loc[\"ctr\"].to_dict()\n",
    "\n",
    "def pattern_counts(titles):\n",
    "    out = {}\n",
    "    for name, fn in [\n",
    "        (\"questions\", lambda t: \"?\" in t),\n",
    "        (\"numbers\",   lambda t: any(c.isdigit() for c in t)),\n",
    "        (\"quotes\",    lambda t: '\"' in t or \"'\" in t),\n",
    "        (\"colons\",    lambda t: \":\" in t),\n",
    "        (\"ellipsis\",  lambda t: \"...\" in t)\n",
    "    ]:\n",
    "        cnt = sum(fn(t) for t in titles)\n",
    "        out[name] = {\"count\": cnt, \"pct\": cnt/len(titles)*100}\n",
    "    return out\n",
    "\n",
    "med = df_train[\"ctr\"].median()\n",
    "high_ids = df_train[df_train[\"ctr\"]>med][\"newsID\"]\n",
    "low_ids  = df_train[df_train[\"ctr\"]<=med][\"newsID\"]\n",
    "\n",
    "high_patterns = pattern_counts(df_train[df_train[\"newsID\"].isin(high_ids)][\"title\"].sample(5000, random_state=42))\n",
    "low_patterns  = pattern_counts(df_train[df_train[\"newsID\"].isin(low_ids)][\"title\"].sample(5000, random_state=42))\n",
    "\n",
    "guidelines = {\n",
    "    \"category_performance\":     cat_stats,\n",
    "    \"reading_ease_correlation\": corrs[\"title_reading_ease\"],\n",
    "    \"headline_patterns\": {\n",
    "        \"high_engagement\": high_patterns,\n",
    "        \"low_engagement\":  low_patterns\n",
    "    },\n",
    "    \"overall_ctr\": df_train[\"ctr\"].mean()\n",
    "}\n",
    "\n",
    "with open(PREP_DIR/\"processed_data\"/\"editorial_guidelines.json\", \"w\") as f:\n",
    "    json.dump(guidelines, f, indent=2)\n",
    "\n",
    "print(\"Editorial guidelines saved to\", PREP_DIR/\"processed_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
